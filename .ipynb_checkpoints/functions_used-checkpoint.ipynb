{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "#reshape data for models other than CNN and VGG\n",
    "def reshape_data_general(raw_data):\n",
    "    y = copy.deepcopy(raw_data['emotion'])\n",
    "    data = raw_data.drop(columns = ['emotion'])\n",
    "    x = []\n",
    "    for i in range(data.shape[0]):\n",
    "        sample = copy.deepcopy(data.loc[i,' pixels'])\n",
    "        sample = sample.split(' ')\n",
    "        sample = np.array(list(map(int, sample)))/255-1\n",
    "        x.append(sample)\n",
    "    x = np.array(x)\n",
    "    return x,y\n",
    "#reshape data for CNN and VGG\n",
    "def reshape_data_CNN(raw_data):\n",
    "    y = copy.deepcopy(raw_data['emotion'])\n",
    "    data = raw_data.drop(columns = ['emotion'])\n",
    "    x = []\n",
    "    for i in range(data.shape[0]):\n",
    "        sample = copy.deepcopy(data.loc[i,' pixels'])\n",
    "        sample = sample.split(' ')\n",
    "        sample = np.array(list(map(int, sample)))/255-1\n",
    "        sample = sample.reshape((48,48,1))\n",
    "        x.append(sample)\n",
    "    x = np.array(x)\n",
    "    y = y.to_frame()\n",
    "    y = pd.get_dummies(y,columns = ['emotion'])\n",
    "    return x,y\n",
    "#generate sample images\n",
    "def generate_image(pixels):\n",
    "    plt.imshow(pixels)\n",
    "    plt.show()\n",
    "#generate training class weights used for fitting CNN and VGG\n",
    "def get_class_weights(data):\n",
    "    uniques = data.emotion.unique()\n",
    "    dist_dict = {}\n",
    "    for i in uniques:\n",
    "        dist_dict[i] = data[data.emotion == i].shape[0]/data.shape[0]\n",
    "    return dist_dict\n",
    "#graph CNN and VGG model metrics\n",
    "def metrics_grapher_CNN(history):\n",
    "    length = int(len(history.history)/2)\n",
    "    key_list = list(history.history.keys())\n",
    "    for i in range(len(key_list)):\n",
    "        if i==length:\n",
    "            break\n",
    "        plt.plot(history.history[key_list[i]])\n",
    "        plt.plot(history.history[key_list[i+length]])\n",
    "        plt.title('model '+key_list[i])\n",
    "        plt.ylabel(key_list[i])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()    \n",
    "# print CNN and VGG model summary\n",
    "def model_summary_CNN(model):\n",
    "    model.summary()\n",
    "    return None\n",
    "#convert index labels into string for CNN and VGG\n",
    "def convert_labels_CNN(ypred,ytrue,label_list):\n",
    "    pred_indices = np.argmax(ypred,axis = 1)\n",
    "    true_indices = np.argmax(ytrue,axis = 1)\n",
    "    pred = []\n",
    "    true = []\n",
    "    for i in range(pred_indices.shape[0]):\n",
    "        pred.append(label_list[pred_indices[i]])\n",
    "        true.append(label_list[true_indices[i]])\n",
    "    return pred,true\n",
    "#convert index labels into string for other models\n",
    "def convert_labels_general(ypred,ytrue,label_list):\n",
    "    pred = []\n",
    "    true = []\n",
    "    for i in range(ypred.shape[0]):\n",
    "        pred.append(label_list[ypred[i]])\n",
    "        true.append(label_list[ytrue[i]])\n",
    "    return pred,true\n",
    "#draw confusion matrix\n",
    "def draw_confusion(true,pred,label_list):\n",
    "    cm = confusion_matrix(true,pred,labels = label_list)\n",
    "    confusionMatrix = pd.DataFrame(cm,\n",
    "                         index = label_list, \n",
    "                         columns = label_list)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(confusionMatrix, annot=True, fmt='g', ax=ax)\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    return None\n",
    "#display statistics for CNN and VGG model\n",
    "def describe_CNN(model,history,xtest,ytest,label_list,is_vgg16):\n",
    "    metrics_grapher_CNN(history)\n",
    "    model_summary_CNN(model)\n",
    "    if is_vgg16:\n",
    "        xtest = np.repeat(xtest, repeats=3, axis=3)\n",
    "    ypred_CNN = model.predict(xtest)\n",
    "    CNNpred,true = convert_labels_CNN(ypred_CNN,ytest.values,label_list)\n",
    "    print(classification_report(true, CNNpred))\n",
    "    draw_confusion(true,CNNpred,label_list)\n",
    "    return None\n",
    "#display statistics for other models\n",
    "def describe_general(model,xtest,ytest,label_list):\n",
    "    ypred = model.predict(xtest)\n",
    "    pred,true = convert_labels_general(ypred,ytest.to_numpy(),label_list)\n",
    "    print(classification_report(true, pred))\n",
    "    draw_confusion(true,pred,label_list)\n",
    "    return accuracy_score(pred,true)\n",
    "#describe data\n",
    "def describe_data(training,validation,testing,xtrain,label_list):\n",
    "    print('train size: '+str(training.shape[0]))\n",
    "    print('validation size: '+str(validation.shape[0]))\n",
    "    print('test size: '+str(testing.shape[0]))\n",
    "    training_label = []\n",
    "    valid_label = []\n",
    "    testing_label = []\n",
    "    for i in range(training.shape[0]):\n",
    "        training_label.append(label_list[training['emotion'][i]])\n",
    "    for i in range(testing.shape[0]):\n",
    "        testing_label.append(label_list[testing['emotion'][i]])\n",
    "    for i in range(validation.shape[0]):\n",
    "        valid_label.append(label_list[validation['emotion'][i]])\n",
    "    print('training set class distribution: ')\n",
    "    plt.hist(np.array(training_label),bins = 7)\n",
    "    plt.show()\n",
    "    print('testing set class distribution: ')\n",
    "    plt.hist(np.array(testing_label),bins = 7)\n",
    "    plt.show()\n",
    "    print('validation set class distribution: ')\n",
    "    plt.hist(np.array(valid_label),bins = 7)\n",
    "    plt.show()\n",
    "    for i in label_list:\n",
    "        print(i)\n",
    "        generate_image(xtrain[training_label.index(i)])\n",
    "        plt.show()\n",
    "    return None\n",
    "#definte VGG model\n",
    "def VGG16_model(xtrain,ytrain,xval,yval,class_weights,filter_num,filter_size,pooling_size,dropout_rate):\n",
    "    xtrain_colored = np.repeat(xtrain, repeats=3, axis=3)\n",
    "    xval_colored = np.repeat(xval, repeats=3, axis=3)\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape= (48, 48, 3))\n",
    "    base_model.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile( 'adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,verbose = 1)\n",
    "    history = model.fit(xtrain_colored, ytrain,validation_data=(xval_colored, yval),class_weight = class_weights,epochs=100,\n",
    "                            batch_size=100, callbacks=[callback])\n",
    "    return model,history\n",
    "#define CNN model\n",
    "#conv - pooling - conv- pooling- flatten - dense - dense\n",
    "def CNN_model(xtrain,ytrain,xval,yval,class_weights,filter_num,filter_size,pooling_size,dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = filter_num, kernel_size =  filter_size, activation='relu',padding=\"same\" ,\n",
    "                     input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=pooling_size))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Conv2D(filters = filter_num, kernel_size = filter_size,padding=\"same\" , activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pooling_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile( 'adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,verbose = 1)\n",
    "    history = model.fit(xtrain, ytrain,validation_data=(xval, yval),class_weight = class_weights,epochs=100,\n",
    "                        batch_size=100, callbacks=[callback])\n",
    "    return model,history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
